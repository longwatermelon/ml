\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{indentfirst}
\usepackage{amsfonts}
\geometry{legalpaper, portrait, margin=0.5in}
\usepackage{color}   %May be necessary if you want to color links
\usepackage{hyperref}
\hypersetup{
    colorlinks=true, %set true if you want colored links
    linktoc=all,     %set to all if you want both sections and subsections linked
    linkcolor=black,  %choose some color if you want links to stand out
}
\graphicspath{ {./images/} }
\begin{document}
\newcommand*\dif{\mathop{}\!\mathrm{d}}

\newenvironment{myitemize}
{ \begin{itemize}
    \setlength{\itemsep}{0pt}
    \setlength{\parskip}{0pt}
    \setlength{\parsep}{0pt}     }
{ \end{itemize}                  } 

\newenvironment{myenumerate}
{ \begin{enumerate}
    \setlength{\itemsep}{0pt}
    \setlength{\parskip}{0pt}
    \setlength{\parsep}{0pt}     }
{ \end{enumerate}                  } 

\begin{titlepage}
\begin{center}
\vspace*{2cm}
\begin{huge}\textbf{Machine Learning}\end{huge}
\end{center}
\end{titlepage}

\tableofcontents

\addcontentsline{toc}{section}{Table of contents}

\pagebreak

\section{Regression}

\subsection{Linear regression}
\subsubsection{Squared error cost function}

Measures how well line fits training data

$m$ = num of training examples

$\vec{x}$ holds training example x values (length $m$)

$\vec{y}$ holds training example y values (length $m$)

${\hat y}^{(i)}$ = $w\vec{x}^{(i)} + b$

\[ J(w,b) = \frac{1}{2m} \sum_{i=1}^m ({\hat y}^{(i)} - \vec{y}_i)^2 \]

$\frac{1}{m}$ finds average error for larger data sets, $\frac{1}{2m}$ makes later calculations neater

\subsubsection{Gradient descent}

Find $w,b$ for minimum of cost function $J(w,b)$

\begin{myenumerate}
	\item Start with some $w,b$ (commonly $0,0$)
	\item Look around starting point and find direction that will move the point furthest downwards for a small step size
\end{myenumerate}

$\alpha$ = learning rate

Must simultaneously update $w$ and $b$

\begin{align*}
    w_1 &= w_0 - \alpha \frac{\partial}{\partial w} J(w_0,b_0)\\
    b_1 &= b_0 - \alpha \frac{\partial}{\partial b} J(w_0,b_0)\\
    \frac{\partial}{\partial w} J(w,b) &= \frac{1}{m} \sum_{i=1}^m ({\hat y}^{(i)} - \vec{y}_i) \vec{x}^{(i)}\\
    \frac{\partial}{\partial b} J(w,b) &= \frac{1}{m} \sum_{i=1}^m ({\hat y}^{(i)} - \vec{y}_i)
\end{align*}

\subsection{Multiple linear regression}

$n_f$ = number of features

$m$ = number of data points

$\vec{w}$ = vector of weights (length $n_f$)

$X$ is a list of x vectors which hold $n_f$ features (size $m \times n_f$)

Sum of predictions of all features is the prediction of multiple linear reg

\begin{align*}
    f_{\vec{w},b}(\vec{x}) &= \vec{w} \cdot \vec{x} + b
\end{align*}

Gradient descent

\begin{align*}
    \vec{w}_j &= \vec{w}_j - \alpha \frac{\partial}{\partial \vec{w}_j} J(\vec{w},b)\\
    b &= b - \alpha \frac{\partial}{\partial b} J(\vec{w},b)
\end{align*}

Cost function and its partial derivatives

\begin{align*}
    J(\vec{w},b) &= \frac{1}{2m} \sum_{i=1}^{m} (f_{\vec{w},b}(X^{(i)}) - \vec{y}_i)^2\\
    \frac{\partial}{\partial \vec{w}_j} J(\vec{w},b) &= \frac{1}{m} \sum_{i=1}^{m} (f_{\vec{w},b}(X^{(i)}) - \vec{y}_i) X_j^{(i)}\\
    \frac{\partial}{\partial b} J(\vec{w},b) &= \frac{1}{m} \sum_{i=1}^{m} (f_{\vec{w},b}(X^{(i)}) - \vec{y}_i)
\end{align*}

\subsection{Logistic regression}

Sigmoid function

\begin{align*}
    g(z) &= \frac{1}{1 + e^{-z}}\\
    z &= f_{\vec{w},b}(\vec{x})\\
    \hat{y}^{(i)} &= g(f_{\vec{w},b}(X^{(i)}))
\end{align*}

$\hat{y}^{(i)}$ can be interpreted as the "probability" that class is 1, $0 \leq \hat{y}^{(i)} \leq 1$

ex. $\hat{y}^{(i)} = 0.7$ means there is a 70\% chance $y$ is 1

Logistic regression requires a new cost function because $f_{\vec{w},b}(\vec{x})$ for logistic regression is non-convex, trapping gradient descend in local minima.

Cost function

\[ J(\vec{w},b) = \frac{1}{m} \sum_{i=1}^{m} L(\hat{y}^{(i)},\vec{y}_i) \]

\begin{equation*}
L(\hat{y}^{(i)},\vec{y}_i) = 
  \left\{
    \begin{aligned}
      & -\log(\hat{y}^{(i)}) & \text{if } \vec{y}_i = 1 \\
      & -\log(1 - \hat{y}^{(i)}) & \text{if } \vec{y}_i = 0
    \end{aligned}
  \right.
\end{equation*}

Simplified form
\[ L(\hat{y}^{(i)}, \vec{y}_i) = -\vec{y}_i \log(\hat{y}^{(i)}) - (1 - \vec{y}_i) \log (1 - \hat{y}^{(i)}) \]

The loss function will decrease as $\hat{y}^{(i)}$ approaches $\vec{y}_i$ on a graph of $L$ vs $f$.

$\frac{\partial J(\vec{w},b)}{\partial \vec{w}_j}$ and $\frac{\partial J(\vec{w},b)}{\partial b}$ are the same as in linear regression, just the definition of $f$ has changed.

\subsection{Softmax regression}

Generalization of logistic regression, $y$ can have more than two possible values.

The most probable value of $y$ is the value that when given to $L$ yields the largest loss.

Calculate $z_i$ with $\vec{x}$ only consisting of data points that have label $i$. In implementation, set all $y$ values of data points with label equal to $i$ to 1, and 0 for everything else.

$n_f$ = num features

$n_y$ = number of possible $y$ outputs

$W$ is a matrix of dimensions $n_y \times n_f$.

$\vec{b}$, $\vec{z}$, $\vec{a}$ are vectors of length $n_y$.

\begin{gather*}
    1 \leq i \leq n_y\\
    \vec{z}_i = W^{(i)} \cdot \vec{x} + \vec{b}_i\\
    \vec{a}_i = \frac{e^{\vec{z}_i}}{\sum_{k=1}^{n_y} e^{\vec{z}_k}}
\end{gather*}

\begin{equation}
L(\vec{a}, y) =
  \left\{
    \begin{aligned}
    -\log \vec{a}_1 &\text{ if } y = 1\\
    -\log \vec{a}_2 &\text{ if } y = 2\\
    & \vdots\\
    -\log \vec{a}_n &\text{ if } y = n
    \end{aligned}
   \right.
\end{equation}

\subsection{Feature scaling: z-score normalization}

After z-score normalization, all features will have a mean of 0 and a standard deviation of 1

$n_f$ = num features

$\vec{\mu}_j$ = mean of all values for feature $j$ (length $n_f$)

$\vec{\sigma}_j$ = standard deviation of feature $j$ (length $n_f$)

\begin{align*}
    X_j^{(i)} &= \frac{X_j^{(i)} - \vec{\mu}_j}{\vec{\sigma}_j}\\
    \vec{\mu}_j &= \frac{1}{m} \sum_{i=0}^{m-1} X_j^{(i)}\\
    \vec{\sigma}_j^2 &= \frac{1}{m} \sum_{i=0}^{m-1} (X_j^{(i)} - \vec{\mu}_j)^2
\end{align*}

\subsection{Over / underfitting}

Underfit / high bias: does not fit training set well ($wx + b$ fit onto data points with $x + x^2$ shape)

Overfit / high variance: fits training set extremely well but does not generalize well ($w_1 x + w_2 x^2 + w_3 x^3 + w_4 x^4 + b$ fit onto training set of shape $x + x^2$ can have zero cost but predicts values outside the training set inaccurately)

\vspace{5px}

Addressing overfitting
\begin{myitemize}
	\item Collect more data
	\item Select features ("Feature selection")
	\item Reduce size of parameters ("Regularization")
\end{myitemize}

\subsubsection{Regularization}

Small values of $w_1,w_2,\cdots,w_n,b$ for simpler model, less likely to overfit

Given $n_f$ features, there is no way to tell which features are important and which features should be penalized, so all features are penalized.
\[ J_r(\vec{w},b) = J(\vec{w},b) + \frac{\lambda}{2m} \sum_{j=1}^{n_f} \vec{w}_j^2 \]

Can include $b$ by adding $\frac{\lambda}{2m} b^2$ to $J_r$ but typically doesn't make a large difference.

The extra term in $J_r$ is called the regularization term.

Effectively, $\lambda \propto \frac{1}{w}$. When trying to minimize cost, either the error term or the regularization term must decrease. The larger the lambda, the more the regularization term should decrease to minimize cost, decreasing $w$ parameters.

\noindent \textbf{Regularized linear regression}

\[ J_r(\vec{w},b) = \frac{1}{2m} \sum_{i=1}^m \left[(f_{\vec{w},b}(X^{(i)}) - \vec{y}_i)^2\right] + \frac{\lambda}{2m} \sum_{j=1}^{n_f} \vec{w}_j^2 \]

For gradient descent, only $\frac{\partial J_r}{\partial \vec{w}_j}$ changes ($b$ is not regularized):

\[ \frac{\partial J_r}{\partial \vec{w}_j} = \frac{1}{m} \sum_{i=1}^m \left[(f_{\vec{w},b}(X^{(i)}) - \vec{y}_i)X_j^{(i)}\right] + \frac{\lambda}{m} \vec{w}_j \]

\noindent \textbf{Regularized logistic regression}

\[ J_r(\vec{w},b) = \frac{1}{m} \sum_{i=1}^m L(f_{\vec{w},b}(X^{(i)}), \vec{y}_i) + \frac{\lambda}{2m} \sum_{j=1}^{n_f} \vec{w}_j^2 \]

For gradient descent, only $\frac{\partial J_r}{\partial w_j}$ changes (b is not regularized):

\[ \frac{\partial J_r}{\partial \vec{w}_j} = \frac{1}{m} \sum_{i=1}^m \left[(f_{\vec{w},b}(X^{(i)}) - \vec{y}_i)X_j^{(i)}\right] + \frac{\lambda}{m} \vec{w}_j \]

\pagebreak

\section{Neural networks}

$n_{\ell}$ = num layers excluding input

$n^{[\ell]}_n$ = n neurons in layer $\ell$

$n_f$ = num features

$\vec{W}$ is a vector (length $n_{\ell}$) of matrices of size $n^{[\ell]}_n \times n^{[\ell-1]}_n$

$\vec{x}$ is a vector of outputs from each neuron in previous layer

$\vec{b}$ is a vector (length $n_{\ell}$), holds a bias value for each layer

$Z$ and $A$: vector (length $n_{\ell}$) of vectors (length $n^{[\ell]}_n$)

$g$: activation function

$1 \leq i \leq n_{\ell}$

$a$ (activation) = scalar output of a single neuron

Superscript $[i]$ is used to notate information relating to the $i$th layer in a neural network.

\subsection{Choosing an activation function}

sigmoid: $g(z) = \frac{1}{1 + e^{-z}}$

tanh: $g(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$

linear: $g(z) = z$

ReLU: $g(z) = \max(0, z)$

Leaky ReLU: $g(z) = \max(\epsilon z, z)$ where $\epsilon$ is a small nonzero positive value $<$ 1

\subsubsection*{For output layer}

Binary classification, y = 0 or 1: use sigmoid

Regression, $-\infty \leq y \leq \infty$: use linear activation function

Regression, y $\geq$ 0: use ReLU

\subsubsection*{For hidden layer}

ReLU is most common

\subsection{Training a model}
\subsubsection{Forward propagation}

Input $A^{[\ell-1]}$, output $A^{[\ell]}$, cache $Z^{[\ell]}$, $W^{[\ell]}$, $\vec{b}^{[\ell]}$

\begin{align*}
    Z^{[\ell]} &= W^{[\ell]} A^{[\ell-1]} + \vec{b}^{[\ell]}\\
    A^{[\ell]} &= g^{[\ell]}(Z^{[\ell]})
\end{align*}

Up to $A^{[n_{\ell}]}$, in which case $\hat{y} = A^{[n_{\ell}]}_0$ assuming output layer has one unit

\subsubsection{Back propagation}

Input $\dif a^{[\ell]}$, output $\dif a^{[\ell-1]}$, $\dif W^{[\ell-1]}$, $\dif \vec{b}^{[\ell-1]}$

\begin{align*}
    \dif Z^{[\ell]} &= \dif A^{[\ell]} \cdot g'^{[\ell]}(Z^{[\ell]})\\
    \dif W^{[\ell]} &= \frac{1}{m} \dif Z^{[\ell]} \cdot A^{[\ell-1] T}\\
    \dif \vec{b}^{[\ell]} &= \frac{1}{m} \sum_i \dif Z^{[\ell]}_i\\
    \dif A^{[\ell-1]} &= W^{[\ell] T} \cdot \dif Z^{[\ell]}
\end{align*}

\subsection{Improving model}

Cross validation: split data into training and test, use test data to determine how well the model generalizes

\subsubsection{Fixing high bias/variance}

High bias (underfit): $J_{train}$ high, $J_{train} \approx J_{cv}$

High variance (overfit): $J_{train}$ may be low, $J_{cv} \gg J_{train}$

High bias and high variance: $J_{train}$ high, $J_{cv} \gg J_{train}$

How to fix:

\begin{myenumerate}
\item Get more training examples (fix high variance)
\item Try smaller sets of features (fix high variance)
\item Add more features (fix high bias)
\item Add polynomial features (fix high bias)
\item Decrease $\lambda$ (fix high bias)
\item Increase $\lambda$ (fix high variance)
\end{myenumerate}

\textbf{Neural networks and bias/variance}

If $J_{train}$ is high, make the network larger

If $J_{cv}$ is high, get more data

\subsubsection{Adding data}

Data augmentation: add data with distortions (ex. distorted letters in a letter recognition program)

\subsection{Decision trees}

\end{document}
