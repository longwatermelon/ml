\contentsline {section}{Table of contents}{1}{section*.1}%
\contentsline {section}{\numberline {1}Regression}{2}{section.1}%
\contentsline {subsection}{\numberline {1.1}Linear regression}{2}{subsection.1.1}%
\contentsline {subsubsection}{\numberline {1.1.1}Squared error cost function}{2}{subsubsection.1.1.1}%
\contentsline {subsubsection}{\numberline {1.1.2}Gradient descent}{2}{subsubsection.1.1.2}%
\contentsline {subsection}{\numberline {1.2}Multiple linear regression}{2}{subsection.1.2}%
\contentsline {subsection}{\numberline {1.3}Logistic regression}{3}{subsection.1.3}%
\contentsline {subsection}{\numberline {1.4}Softmax regression}{3}{subsection.1.4}%
\contentsline {subsection}{\numberline {1.5}Feature scaling: z-score normalization}{3}{subsection.1.5}%
\contentsline {subsection}{\numberline {1.6}Over / underfitting}{4}{subsection.1.6}%
\contentsline {subsubsection}{\numberline {1.6.1}Regularization}{4}{subsubsection.1.6.1}%
\contentsline {section}{\numberline {2}Neural networks}{5}{section.2}%
\contentsline {subsection}{\numberline {2.1}Choosing an activation function}{5}{subsection.2.1}%
\contentsline {subsection}{\numberline {2.2}Training a model}{5}{subsection.2.2}%
\contentsline {subsubsection}{\numberline {2.2.1}Forward propagation}{5}{subsubsection.2.2.1}%
\contentsline {subsubsection}{\numberline {2.2.2}Back propagation}{5}{subsubsection.2.2.2}%
\contentsline {subsection}{\numberline {2.3}Improving model}{5}{subsection.2.3}%
\contentsline {subsubsection}{\numberline {2.3.1}Fixing high bias/variance}{6}{subsubsection.2.3.1}%
\contentsline {subsubsection}{\numberline {2.3.2}Adding data}{6}{subsubsection.2.3.2}%
\contentsline {section}{\numberline {3}Decision trees}{6}{section.3}%
\contentsline {subsection}{\numberline {3.1}Measuring purity}{6}{subsection.3.1}%
\contentsline {subsubsection}{\numberline {3.1.1}Entropy as a measure of impurity}{6}{subsubsection.3.1.1}%
\contentsline {subsection}{\numberline {3.2}Choosing a split}{7}{subsection.3.2}%
\contentsline {subsection}{\numberline {3.3}Constructing a decision tree}{7}{subsection.3.3}%
\contentsline {subsection}{\numberline {3.4}Features with multiple possible values}{8}{subsection.3.4}%
\contentsline {subsection}{\numberline {3.5}Tree ensembles}{8}{subsection.3.5}%
\contentsline {subsubsection}{\numberline {3.5.1}Sampling with replacement}{8}{subsubsection.3.5.1}%
\contentsline {subsubsection}{\numberline {3.5.2}Random forest algorithm}{8}{subsubsection.3.5.2}%
\contentsline {subsubsection}{\numberline {3.5.3}XGBoost}{8}{subsubsection.3.5.3}%
\contentsline {section}{\numberline {4}Unsupervised learning}{9}{section.4}%
\contentsline {subsection}{\numberline {4.1}Clustering: K-means}{9}{subsection.4.1}%
\contentsline {subsubsection}{\numberline {4.1.1}Algorithm}{9}{subsubsection.4.1.1}%
\contentsline {subsubsection}{\numberline {4.1.2}Cost function}{9}{subsubsection.4.1.2}%
\contentsline {subsubsection}{\numberline {4.1.3}Choosing k}{9}{subsubsection.4.1.3}%
\contentsline {subsection}{\numberline {4.2}Anomaly detection}{10}{subsection.4.2}%
\contentsline {subsubsection}{\numberline {4.2.1}Normal distribution}{10}{subsubsection.4.2.1}%
\contentsline {subsubsection}{\numberline {4.2.2}Density estimation}{10}{subsubsection.4.2.2}%
\contentsline {subsection}{\numberline {4.3}Recommender systems}{10}{subsection.4.3}%
\contentsline {subsubsection}{\numberline {4.3.1}Collaborative filtering}{11}{subsubsection.4.3.1}%
\contentsline {subsubsection}{\numberline {4.3.2}Cost function}{11}{subsubsection.4.3.2}%
\contentsline {subsubsection}{\numberline {4.3.3}Binary labels}{11}{subsubsection.4.3.3}%
\contentsline {subsubsection}{\numberline {4.3.4}Mean normalization}{11}{subsubsection.4.3.4}%
\contentsline {subsubsection}{\numberline {4.3.5}Finding related items}{12}{subsubsection.4.3.5}%
\contentsline {subsubsection}{\numberline {4.3.6}Content based filtering}{12}{subsubsection.4.3.6}%
