\contentsline {section}{Table of contents}{2}{section*.1}%
\contentsline {section}{\numberline {1}Regression}{3}{section.1}%
\contentsline {subsection}{\numberline {1.1}Linear regression}{3}{subsection.1.1}%
\contentsline {subsubsection}{\numberline {1.1.1}Squared error cost function}{3}{subsubsection.1.1.1}%
\contentsline {subsubsection}{\numberline {1.1.2}Gradient descent}{3}{subsubsection.1.1.2}%
\contentsline {subsection}{\numberline {1.2}Multiple linear regression}{3}{subsection.1.2}%
\contentsline {subsection}{\numberline {1.3}Logistic regression}{4}{subsection.1.3}%
\contentsline {subsection}{\numberline {1.4}Softmax regression}{4}{subsection.1.4}%
\contentsline {subsection}{\numberline {1.5}Feature scaling: z-score normalization}{4}{subsection.1.5}%
\contentsline {subsection}{\numberline {1.6}Over / underfitting}{5}{subsection.1.6}%
\contentsline {subsubsection}{\numberline {1.6.1}Regularization}{5}{subsubsection.1.6.1}%
\contentsline {section}{\numberline {2}Neural networks}{6}{section.2}%
\contentsline {subsection}{\numberline {2.1}Intuition}{6}{subsection.2.1}%
\contentsline {subsection}{\numberline {2.2}Activation functions}{6}{subsection.2.2}%
\contentsline {subsection}{\numberline {2.3}Training the model}{6}{subsection.2.3}%
\contentsline {subsubsection}{\numberline {2.3.1}Forward propagation}{6}{subsubsection.2.3.1}%
\contentsline {subsubsection}{\numberline {2.3.2}Back propagation}{7}{subsubsection.2.3.2}%
\contentsline {subsubsection}{\numberline {2.3.3}Back propagation derivation}{7}{subsubsection.2.3.3}%
\contentsline {subsection}{\numberline {2.4}Convolutional neural network}{8}{subsection.2.4}%
\contentsline {subsubsection}{\numberline {2.4.1}Edge detection}{8}{subsubsection.2.4.1}%
\contentsline {subsubsection}{\numberline {2.4.2}Padding}{8}{subsubsection.2.4.2}%
\contentsline {subsubsection}{\numberline {2.4.3}Strided convolution}{8}{subsubsection.2.4.3}%
\contentsline {subsubsection}{\numberline {2.4.4}Convolution over a volume}{9}{subsubsection.2.4.4}%
\contentsline {subsubsection}{\numberline {2.4.5}One layer of a cnn}{9}{subsubsection.2.4.5}%
\contentsline {subsubsection}{\numberline {2.4.6}Cnn notation}{9}{subsubsection.2.4.6}%
\contentsline {subsubsection}{\numberline {2.4.7}Example cnn (not vectorized)}{9}{subsubsection.2.4.7}%
\contentsline {subsubsection}{\numberline {2.4.8}Pooling layers}{10}{subsubsection.2.4.8}%
\contentsline {subsubsection}{\numberline {2.4.9}Fully connected layer}{10}{subsubsection.2.4.9}%
\contentsline {subsubsection}{\numberline {2.4.10}Complete cnn example}{10}{subsubsection.2.4.10}%
\contentsline {subsubsection}{\numberline {2.4.11}Forward prop}{10}{subsubsection.2.4.11}%
\contentsline {subsubsection}{\numberline {2.4.12}Why convolutions}{10}{subsubsection.2.4.12}%
\contentsline {subsection}{\numberline {2.5}Improving model}{11}{subsection.2.5}%
\contentsline {subsubsection}{\numberline {2.5.1}Fixing high bias/variance}{11}{subsubsection.2.5.1}%
\contentsline {subsubsection}{\numberline {2.5.2}Adding data}{11}{subsubsection.2.5.2}%
\contentsline {section}{\numberline {3}Cnn back prop}{11}{section.3}%
\contentsline {subsection}{\numberline {3.1}Deriving $\frac {dL}{db}$ for forward feeding into dense layer}{11}{subsection.3.1}%
\contentsline {subsubsection}{\numberline {3.1.1}$\frac {dL}{dA^l_{enuv}}$}{11}{subsubsection.3.1.1}%
\contentsline {subsubsection}{\numberline {3.1.2}$\frac {dA^l_{enuv}}{dZ^l_{enuv}}$}{12}{subsubsection.3.1.2}%
\contentsline {subsubsection}{\numberline {3.1.3}$\frac {dZ^l_{enuv}}{db^l_n}$}{12}{subsubsection.3.1.3}%
\contentsline {section}{\numberline {4}Decision trees}{13}{section.4}%
\contentsline {subsection}{\numberline {4.1}Measuring purity}{13}{subsection.4.1}%
\contentsline {subsubsection}{\numberline {4.1.1}Entropy as a measure of impurity}{13}{subsubsection.4.1.1}%
\contentsline {subsection}{\numberline {4.2}Choosing a split}{13}{subsection.4.2}%
\contentsline {subsection}{\numberline {4.3}Constructing a decision tree}{14}{subsection.4.3}%
\contentsline {subsection}{\numberline {4.4}Features with multiple possible values}{14}{subsection.4.4}%
\contentsline {subsection}{\numberline {4.5}Tree ensembles}{15}{subsection.4.5}%
\contentsline {subsubsection}{\numberline {4.5.1}Sampling with replacement}{15}{subsubsection.4.5.1}%
\contentsline {subsubsection}{\numberline {4.5.2}Random forest algorithm}{15}{subsubsection.4.5.2}%
\contentsline {subsubsection}{\numberline {4.5.3}XGBoost}{15}{subsubsection.4.5.3}%
\contentsline {section}{\numberline {5}Unsupervised learning}{15}{section.5}%
\contentsline {subsection}{\numberline {5.1}Clustering: K-means}{15}{subsection.5.1}%
\contentsline {subsubsection}{\numberline {5.1.1}Algorithm}{15}{subsubsection.5.1.1}%
\contentsline {subsubsection}{\numberline {5.1.2}Cost function}{16}{subsubsection.5.1.2}%
\contentsline {subsubsection}{\numberline {5.1.3}Choosing k}{16}{subsubsection.5.1.3}%
\contentsline {subsection}{\numberline {5.2}Anomaly detection}{16}{subsection.5.2}%
\contentsline {subsubsection}{\numberline {5.2.1}Normal distribution}{16}{subsubsection.5.2.1}%
\contentsline {subsubsection}{\numberline {5.2.2}Density estimation}{17}{subsubsection.5.2.2}%
\contentsline {subsection}{\numberline {5.3}Recommender systems}{17}{subsection.5.3}%
\contentsline {subsubsection}{\numberline {5.3.1}Collaborative filtering}{18}{subsubsection.5.3.1}%
\contentsline {subsubsection}{\numberline {5.3.2}Cost function}{18}{subsubsection.5.3.2}%
\contentsline {subsubsection}{\numberline {5.3.3}Binary labels}{18}{subsubsection.5.3.3}%
\contentsline {subsubsection}{\numberline {5.3.4}Mean normalization}{18}{subsubsection.5.3.4}%
\contentsline {subsubsection}{\numberline {5.3.5}Finding related items}{19}{subsubsection.5.3.5}%
\contentsline {subsubsection}{\numberline {5.3.6}Content based filtering}{19}{subsubsection.5.3.6}%
\contentsline {subsection}{\numberline {5.4}Reinforcement learning}{19}{subsection.5.4}%
\contentsline {subsubsection}{\numberline {5.4.1}State action value function}{19}{subsubsection.5.4.1}%
